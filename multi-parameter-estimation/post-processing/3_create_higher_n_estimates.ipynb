{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c56f16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934814e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to force a full refresh of the data\n",
    "full_refresh = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702ac223",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = os.popen('git rev-parse --show-toplevel').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ec71d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2025-06-02--17h-06m-16s',\n",
       " '2025-06-02--17h-07m-37s',\n",
       " '2025-06-02--17h-07m-47s',\n",
       " '2025-06-02--17h-07m-56s',\n",
       " '2025-06-02--17h-08m-05s',\n",
       " '2025-06-02--17h-08m-19s']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.path.join(repo_root, 'multi-parameter-estimation', 'data')\n",
    "\n",
    "# Get list of data directories\n",
    "data_dirs = os.listdir(data_folder)\n",
    "data_dirs = [d for d in data_dirs if os.path.isdir(os.path.join(data_folder, d))]\n",
    "\n",
    "# skip old-data\n",
    "if 'old-data' in data_dirs:\n",
    "    data_dirs.remove('old-data')\n",
    "\n",
    "data_dirs.sort()\n",
    "data_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c024a293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2025-06-02--17h-06m-16s',\n",
       " '2025-06-02--17h-07m-37s',\n",
       " '2025-06-02--17h-07m-47s',\n",
       " '2025-06-02--17h-07m-56s',\n",
       " '2025-06-02--17h-08m-05s',\n",
       " '2025-06-02--17h-08m-19s']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_dirs = data_dirs.copy()\n",
    "\n",
    "if not full_refresh:\n",
    "    for d in data_dirs:\n",
    "        if os.path.exists(os.path.join(data_folder, d, \"chunked_coincidences_n=200.csv\")):\n",
    "            new_data_dirs.remove(d)\n",
    "\n",
    "new_data_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d18ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_dir</th>\n",
       "      <th>C</th>\n",
       "      <th>DB_H</th>\n",
       "      <th>DB_V</th>\n",
       "      <th>SB</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-02--17h-06m-16s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-02--17h-06m-16s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-02--17h-06m-16s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-02--17h-06m-16s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-02--17h-06m-16s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>2025-06-02--17h-08m-19s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>2025-06-02--17h-08m-19s</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>2025-06-02--17h-08m-19s</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>39.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>2025-06-02--17h-08m-19s</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>39.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>2025-06-02--17h-08m-19s</td>\n",
       "      <td>1.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>39.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     data_dir    C  DB_H  DB_V    SB     N\n",
       "0     2025-06-02--17h-06m-16s  0.0  40.0   0.0   0.0  40.0\n",
       "1     2025-06-02--17h-06m-16s  0.0  40.0   0.0   0.0  40.0\n",
       "2     2025-06-02--17h-06m-16s  0.0  40.0   0.0   0.0  40.0\n",
       "3     2025-06-02--17h-06m-16s  0.0  40.0   0.0   0.0  40.0\n",
       "4     2025-06-02--17h-06m-16s  0.0  40.0   0.0   0.0  40.0\n",
       "...                       ...  ...   ...   ...   ...   ...\n",
       "1749  2025-06-02--17h-08m-19s  0.0  12.0  12.0  16.0  40.0\n",
       "1750  2025-06-02--17h-08m-19s  1.0  13.0   7.0  19.0  40.0\n",
       "1751  2025-06-02--17h-08m-19s  0.5  12.0  10.0  17.0  39.5\n",
       "1752  2025-06-02--17h-08m-19s  0.5  14.0  12.0  13.0  39.5\n",
       "1753  2025-06-02--17h-08m-19s  1.5   9.0  13.0  16.0  39.5\n",
       "\n",
       "[1754 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_chunks(data_dir):\n",
    "    if not os.path.exists(os.path.join(data_folder, data_dir, \"chunked_coincidences_n=40.csv\")):\n",
    "        print(f\"Skipping {data_dir} as it does not have the required file.\")\n",
    "        return pd.DataFrame()\n",
    "    coincidences = pd.read_csv(os.path.join(data_folder, data_dir, \"chunked_coincidences_n=40.csv\"))\n",
    "    coincidences[\"data_dir\"] = data_dir\n",
    "    return coincidences\n",
    "\n",
    "chunks_df = pd.concat([load_chunks(d) for d in new_data_dirs], ignore_index=True)\n",
    "chunks_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d701e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_348790/345050552.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_80 = chunks_df.groupby('data_dir', group_keys=False).apply(lambda g: k_wise_sum(g, k=2)).reset_index(drop=True)\n",
      "/tmp/ipykernel_348790/345050552.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_120 = chunks_df.groupby('data_dir', group_keys=False).apply(lambda g: k_wise_sum(g, k=3)).reset_index(drop=True)\n",
      "/tmp/ipykernel_348790/345050552.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_160 = chunks_df.groupby('data_dir', group_keys=False).apply(lambda g: k_wise_sum(g, k=4)).reset_index(drop=True)\n",
      "/tmp/ipykernel_348790/345050552.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_200 = chunks_df.groupby('data_dir', group_keys=False).apply(lambda g: k_wise_sum(g, k=5)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_dir</th>\n",
       "      <th>C</th>\n",
       "      <th>DB_H</th>\n",
       "      <th>DB_V</th>\n",
       "      <th>SB</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-02--17h-06m-16s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-02--17h-06m-16s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-02--17h-06m-16s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-02--17h-06m-16s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-02--17h-06m-16s</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>2025-06-02--17h-08m-19s</td>\n",
       "      <td>6.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>79.5</td>\n",
       "      <td>197.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>2025-06-02--17h-08m-19s</td>\n",
       "      <td>3.5</td>\n",
       "      <td>69.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>2025-06-02--17h-08m-19s</td>\n",
       "      <td>5.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>2025-06-02--17h-08m-19s</td>\n",
       "      <td>3.5</td>\n",
       "      <td>64.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>89.5</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>2025-06-02--17h-08m-19s</td>\n",
       "      <td>3.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>198.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    data_dir    C   DB_H  DB_V    SB      N\n",
       "0    2025-06-02--17h-06m-16s  0.0  200.0   0.0   0.0  200.0\n",
       "1    2025-06-02--17h-06m-16s  0.0  200.0   0.0   0.0  200.0\n",
       "2    2025-06-02--17h-06m-16s  0.0  200.0   0.0   0.0  200.0\n",
       "3    2025-06-02--17h-06m-16s  0.0  200.0   0.0   0.0  200.0\n",
       "4    2025-06-02--17h-06m-16s  0.0  200.0   0.0   0.0  200.0\n",
       "..                       ...  ...    ...   ...   ...    ...\n",
       "345  2025-06-02--17h-08m-19s  6.0   72.0  40.0  79.5  197.5\n",
       "346  2025-06-02--17h-08m-19s  3.5   69.0  38.0  87.5  198.0\n",
       "347  2025-06-02--17h-08m-19s  5.0   69.0  42.0  82.0  198.0\n",
       "348  2025-06-02--17h-08m-19s  3.5   64.0  42.0  89.5  199.0\n",
       "349  2025-06-02--17h-08m-19s  3.5   60.0  54.0  81.0  198.5\n",
       "\n",
       "[350 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop columns that are not needed\n",
    "def k_wise_sum(group, k):\n",
    "    # Drop last rows if not divisible by k\n",
    "    n = len(group) - (len(group) % k)\n",
    "    group = group.iloc[:n].reset_index(drop=True)\n",
    "    # Sum every k rows\n",
    "    kwise = group.groupby(group.index // k).sum()\n",
    "    # Restore data_dir from the first row of each group\n",
    "    kwise['data_dir'] = group['data_dir'].iloc[::k].values\n",
    "    return kwise\n",
    "\n",
    "# Example usage for k=3\n",
    "df_80 = chunks_df.groupby('data_dir', group_keys=False).apply(lambda g: k_wise_sum(g, k=2)).reset_index(drop=True)\n",
    "df_120 = chunks_df.groupby('data_dir', group_keys=False).apply(lambda g: k_wise_sum(g, k=3)).reset_index(drop=True)\n",
    "df_160 = chunks_df.groupby('data_dir', group_keys=False).apply(lambda g: k_wise_sum(g, k=4)).reset_index(drop=True)\n",
    "df_200 = chunks_df.groupby('data_dir', group_keys=False).apply(lambda g: k_wise_sum(g, k=5)).reset_index(drop=True)\n",
    "df_200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae418eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes to csv files\n",
    "# save the dataframes to csv files based on the data_dir\n",
    "for data_dir in new_data_dirs:\n",
    "    df_80_subset = df_80[df_80[\"data_dir\"] == data_dir]\n",
    "    df_120_subset = df_120[df_120[\"data_dir\"] == data_dir]\n",
    "    df_160_subset = df_160[df_160[\"data_dir\"] == data_dir]\n",
    "    df_200_subset = df_200[df_200[\"data_dir\"] == data_dir]\n",
    "    df_80_subset.to_csv(os.path.join(data_folder, data_dir, \"chunked_coincidences_n=80.csv\"), index=False)\n",
    "    df_120_subset.to_csv(os.path.join(data_folder, data_dir, \"chunked_coincidences_n=120.csv\"), index=False)\n",
    "    df_160_subset.to_csv(os.path.join(data_folder, data_dir, \"chunked_coincidences_n=160.csv\"), index=False)\n",
    "    df_200_subset.to_csv(os.path.join(data_folder, data_dir, \"chunked_coincidences_n=200.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
